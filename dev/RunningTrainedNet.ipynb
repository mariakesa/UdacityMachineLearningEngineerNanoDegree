{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep Q Network\n",
    "\"\"\"\n",
    "import argparse\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch import optim as optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pl_bolts.datamodules.experience_source import Experience, ExperienceSourceDataset\n",
    "from pl_bolts.losses.rl import dqn_loss\n",
    "from pl_bolts.models.rl.common.gym_wrappers import make_environment\n",
    "from pl_bolts.models.rl.common.memory import MultiStepBuffer\n",
    "from pl_bolts.models.rl.common.networks import CNN\n",
    "\n",
    "from gym import Env\n",
    "\n",
    "from abc import ABC\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Agent(ABC):\n",
    "    \"\"\"Basic agent that always returns 0\"\"\"\n",
    "\n",
    "    def __init__(self, net: nn.Module):\n",
    "        self.net = net\n",
    "\n",
    "    def __call__(self, state: torch.Tensor, device: str, *args, **kwargs) -> List[int]:\n",
    "        \"\"\"\n",
    "        Using the given network, decide what action to carry\n",
    "        Args:\n",
    "            state: current state of the environment\n",
    "            device: device used for current batch\n",
    "        Returns:\n",
    "            action\n",
    "        \"\"\"\n",
    "        return [0]\n",
    "\n",
    "class ValueAgent_(Agent):\n",
    "    \"\"\"Value based agent that returns an action based on the Q values from the network\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        action_space: int,\n",
    "        eps_start: float = 1.0,\n",
    "        eps_end: float = 0.2,\n",
    "        eps_frames: float = 1000,\n",
    "    ):\n",
    "        super().__init__(net)\n",
    "        self.action_space = action_space\n",
    "        self.eps_start = eps_start\n",
    "        self.epsilon = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_frames = eps_frames\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, state: torch.Tensor, device: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Takes in the current state and returns the action based on the agents policy\n",
    "        Args:\n",
    "            state: current state of the environment\n",
    "            device: the device used for the current batch\n",
    "        Returns:\n",
    "            action defined by policy\n",
    "        \"\"\"\n",
    "        if not isinstance(state, list):\n",
    "            state = [state]\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = self.get_random_action(state)\n",
    "        else:\n",
    "            action = self.get_action(state, device)\n",
    "        \n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_random_action(self, state: torch.Tensor) -> int:\n",
    "        \"\"\"returns a random action\"\"\"\n",
    "        actions = []\n",
    "\n",
    "        for i in range(len(state)):\n",
    "            action = np.random.randint(0, self.action_space)\n",
    "            actions.append(action)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def get_action(self, state: torch.Tensor, device: torch.device):\n",
    "        \"\"\"\n",
    "        Returns the best action based on the Q values of the network\n",
    "        Args:\n",
    "            state: current state of the environment\n",
    "            device: the device used for the current batch\n",
    "        Returns:\n",
    "            action defined by Q values\n",
    "        \"\"\"\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, device=device)\n",
    "\n",
    "        q_values = self.net(state)\n",
    "        _, actions = torch.max(q_values, dim=1)\n",
    "        return actions.detach().cpu().numpy()\n",
    "\n",
    "    def update_epsilon(self, step: int) -> None:\n",
    "        \"\"\"\n",
    "        Updates the epsilon value based on the current step\n",
    "        Args:\n",
    "            step: current global step\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.eps_end, self.eps_start - (step + 1) / self.eps_frames)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DQN_(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Basic DQN Model\n",
    "\n",
    "    PyTorch Lightning implementation of `DQN <https://arxiv.org/abs/1312.5602>`_\n",
    "    Paper authors: Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,\n",
    "    Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller.\n",
    "    Model implemented by:\n",
    "\n",
    "        - `Donal Byrne <https://github.com/djbyrne>`\n",
    "\n",
    "    Example:\n",
    "        >>> from pl_bolts.models.rl.dqn_model import DQN\n",
    "        ...\n",
    "        >>> model = DQN(\"PongNoFrameskip-v4\")\n",
    "\n",
    "    Train::\n",
    "\n",
    "        trainer = Trainer()\n",
    "        trainer.fit(model)\n",
    "\n",
    "    Note:\n",
    "        This example is based on:\n",
    "        https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/blob/master/Chapter06/02_dqn_pong.py\n",
    "\n",
    "    Note:\n",
    "        Currently only supports CPU and single GPU training with `distributed_backend=dp`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: str,\n",
    "        eps_start: float = 1.0,\n",
    "        eps_end: float = 0.02,\n",
    "        eps_last_frame: int = 150000,\n",
    "        sync_rate: int = 1000,\n",
    "        gamma: float = 0.99,\n",
    "        learning_rate: float = 1e-4,\n",
    "        batch_size: int = 32,\n",
    "        replay_size: int = 100000,\n",
    "        warm_start_size: int = 10000,\n",
    "        avg_reward_len: int = 100,\n",
    "        min_episode_reward: int = -21,\n",
    "        seed: int = 123,\n",
    "        batches_per_epoch: int = 1000,\n",
    "        n_steps: int = 1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: gym environment tag\n",
    "            eps_start: starting value of epsilon for the epsilon-greedy exploration\n",
    "            eps_end: final value of epsilon for the epsilon-greedy exploration\n",
    "            eps_last_frame: the final frame in for the decrease of epsilon. At this frame espilon = eps_end\n",
    "            sync_rate: the number of iterations between syncing up the target network with the train network\n",
    "            gamma: discount factor\n",
    "            learning_rate: learning rate\n",
    "            batch_size: size of minibatch pulled from the DataLoader\n",
    "            replay_size: total capacity of the replay buffer\n",
    "            warm_start_size: how many random steps through the environment to be carried out at the start of\n",
    "                training to fill the buffer with a starting point\n",
    "            avg_reward_len: how many episodes to take into account when calculating the avg reward\n",
    "            min_episode_reward: the minimum score that can be achieved in an episode. Used for filling the avg buffer\n",
    "                before training begins\n",
    "            seed: seed value for all RNG used\n",
    "            batches_per_epoch: number of batches per epoch\n",
    "            n_steps: size of n step look ahead\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Environment\n",
    "        self.exp = None\n",
    "        self.env = self.make_environment(env, seed)\n",
    "        self.test_env = self.make_environment(env)\n",
    "\n",
    "        self.obs_shape = self.env.observation_space.shape\n",
    "        self.n_actions = self.env.action_space.n\n",
    "\n",
    "        # Model Attributes\n",
    "        self.buffer = None\n",
    "        self.dataset = None\n",
    "\n",
    "        self.net = None\n",
    "        self.target_net = None\n",
    "        self.build_networks()\n",
    "\n",
    "        self.agent = ValueAgent_(\n",
    "            self.net,\n",
    "            self.n_actions,\n",
    "            eps_start=eps_start,\n",
    "            eps_end=eps_end,\n",
    "            eps_frames=eps_last_frame,\n",
    "        )\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.sync_rate = sync_rate\n",
    "        self.gamma = gamma\n",
    "        self.lr = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_size = replay_size\n",
    "        self.warm_start_size = warm_start_size\n",
    "        self.batches_per_epoch = batches_per_epoch\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Metrics\n",
    "        self.total_episode_steps = [0]\n",
    "        self.total_rewards = [0]\n",
    "        self.done_episodes = 0\n",
    "        self.total_steps = 0\n",
    "\n",
    "        # Average Rewards\n",
    "        self.avg_reward_len = avg_reward_len\n",
    "\n",
    "        for _ in range(avg_reward_len):\n",
    "            self.total_rewards.append(torch.tensor(min_episode_reward, device=self.device))\n",
    "\n",
    "        self.avg_rewards = float(np.mean(self.total_rewards[-self.avg_reward_len:]))\n",
    "\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "    def run_n_episodes(self, env, n_epsiodes: int = 1, epsilon: float = 1.0) -> List[int]:\n",
    "        \"\"\"\n",
    "        Carries out N episodes of the environment with the current agent\n",
    "\n",
    "        Args:\n",
    "            env: environment to use, either train environment or test environment\n",
    "            n_epsiodes: number of episodes to run\n",
    "            epsilon: epsilon value for DQN agent\n",
    "        \"\"\"\n",
    "        total_rewards = []\n",
    "\n",
    "        for _ in range(n_epsiodes):\n",
    "            episode_state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                self.agent.epsilon = epsilon\n",
    "                action = self.agent(episode_state, self.device)\n",
    "                #print(action)\n",
    "                next_state, reward, done, _ = env.step(action[0])\n",
    "                episode_state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            total_rewards.append(episode_reward)\n",
    "\n",
    "        return total_rewards\n",
    "\n",
    "    def populate(self, warm_start: int) -> None:\n",
    "        \"\"\"Populates the buffer with initial experience\"\"\"\n",
    "        if warm_start > 0:\n",
    "            self.state = self.env.reset()\n",
    "\n",
    "            for _ in range(warm_start):\n",
    "                self.agent.epsilon = 1.0\n",
    "                action = self.agent(self.state, self.device)\n",
    "                #print(action)\n",
    "                next_state, reward, done, _ = self.env.step(action[0])\n",
    "                exp = Experience(state=self.state, action=action[0], reward=reward, done=done, new_state=next_state)\n",
    "                self.buffer.append(exp)\n",
    "                self.state = next_state\n",
    "\n",
    "                if done:\n",
    "                    self.state = self.env.reset()\n",
    "\n",
    "    def build_networks(self) -> None:\n",
    "        \"\"\"Initializes the DQN train and target networks\"\"\"\n",
    "        self.net = CNN(self.obs_shape, self.n_actions)\n",
    "        self.target_net = CNN(self.obs_shape, self.n_actions)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Passes in a state x through the network and gets the q_values of each action as an output\n",
    "\n",
    "        Args:\n",
    "            x: environment state\n",
    "\n",
    "        Returns:\n",
    "            q values\n",
    "        \"\"\"\n",
    "        output = self.net(x)\n",
    "        return output\n",
    "\n",
    "    def train_batch(self, ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Contains the logic for generating a new batch of data to be passed to the DataLoader\n",
    "\n",
    "        Returns:\n",
    "            yields a Experience tuple containing the state, action, reward, done and next_state.\n",
    "        \"\"\"\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "\n",
    "        while True:\n",
    "            self.total_steps += 1\n",
    "            action = self.agent(self.state, self.device)\n",
    "\n",
    "            next_state, r, is_done, _ = self.env.step(action[0])\n",
    "\n",
    "            episode_reward += r\n",
    "            episode_steps += 1\n",
    "\n",
    "            exp = Experience(state=self.state, action=action[0], reward=r, done=is_done, new_state=next_state)\n",
    "\n",
    "            self.agent.update_epsilon(self.global_step)\n",
    "            self.buffer.append(exp)\n",
    "            self.state = next_state\n",
    "\n",
    "            if is_done:\n",
    "                self.done_episodes += 1\n",
    "                self.total_rewards.append(episode_reward)\n",
    "                self.total_episode_steps.append(episode_steps)\n",
    "                self.avg_rewards = float(np.mean(self.total_rewards[-self.avg_reward_len:]))\n",
    "                self.state = self.env.reset()\n",
    "                episode_steps = 0\n",
    "                episode_reward = 0\n",
    "\n",
    "            states, actions, rewards, dones, new_states = self.buffer.sample(self.batch_size)\n",
    "\n",
    "            for idx, _ in enumerate(dones):\n",
    "                yield states[idx], actions[idx], rewards[idx], dones[idx], new_states[idx]\n",
    "\n",
    "            # Simulates epochs\n",
    "            if self.total_steps % self.batches_per_epoch == 0:\n",
    "                break\n",
    "\n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], _) -> OrderedDict:\n",
    "        \"\"\"\n",
    "        Carries out a single step through the environment to update the replay buffer.\n",
    "        Then calculates loss based on the minibatch recieved\n",
    "\n",
    "        Args:\n",
    "            batch: current mini batch of replay data\n",
    "            _: batch number, not used\n",
    "\n",
    "        Returns:\n",
    "            Training loss and log metrics\n",
    "        \"\"\"\n",
    "\n",
    "        # calculates training loss\n",
    "        loss = dqn_loss(batch, self.net, self.target_net)\n",
    "\n",
    "        if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            loss = loss.unsqueeze(0)\n",
    "\n",
    "        # Soft update of target network\n",
    "        if self.global_step % self.sync_rate == 0:\n",
    "            self.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "        self.log_dict({\n",
    "            \"total_reward\": self.total_rewards[-1],\n",
    "            \"avg_reward\": self.avg_rewards,\n",
    "            \"train_loss\": loss,\n",
    "            \"episodes\": self.done_episodes,\n",
    "            \"episode_steps\": self.total_episode_steps[-1]\n",
    "        })\n",
    "\n",
    "        return OrderedDict({\n",
    "            \"loss\": loss,\n",
    "            \"avg_reward\": self.avg_rewards,\n",
    "        })\n",
    "\n",
    "    def test_step(self, *args, **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Evaluate the agent for 10 episodes\"\"\"\n",
    "        test_reward = self.run_n_episodes(self.test_env, 1, 0)\n",
    "        avg_reward = sum(test_reward) / len(test_reward)\n",
    "        return {\"test_reward\": avg_reward}\n",
    "\n",
    "    def test_epoch_end(self, outputs) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Log the avg of the test results\"\"\"\n",
    "        rewards = [x[\"test_reward\"] for x in outputs]\n",
    "        avg_reward = sum(rewards) / len(rewards)\n",
    "        self.log(\"avg_test_reward\", avg_reward)\n",
    "        return {\"avg_test_reward\": avg_reward}\n",
    "\n",
    "    def configure_optimizers(self) -> List[Optimizer]:\n",
    "        \"\"\" Initialize Adam optimizer\"\"\"\n",
    "        optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        return [optimizer]\n",
    "\n",
    "    def _dataloader(self) -> DataLoader:\n",
    "        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences\"\"\"\n",
    "        self.buffer = MultiStepBuffer(self.replay_size, self.n_steps)\n",
    "        self.populate(self.warm_start_size)\n",
    "\n",
    "        self.dataset = ExperienceSourceDataset(self.train_batch)\n",
    "        return DataLoader(dataset=self.dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get train loader\"\"\"\n",
    "        return self._dataloader()\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get test loader\"\"\"\n",
    "        return self._dataloader()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_environment(env_name: str, seed: Optional[int] = None) -> Env:\n",
    "        \"\"\"\n",
    "        Initialise gym  environment\n",
    "\n",
    "        Args:\n",
    "            env_name: environment name or tag\n",
    "            seed: value to seed the environment RNG for reproducibility\n",
    "\n",
    "        Returns:\n",
    "            gym environment\n",
    "        \"\"\"\n",
    "        env = make_environment(env_name)\n",
    "\n",
    "        if seed:\n",
    "            env.seed(seed)\n",
    "\n",
    "        return env\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(arg_parser: argparse.ArgumentParser, ) -> argparse.ArgumentParser:\n",
    "        \"\"\"\n",
    "        Adds arguments for DQN model\n",
    "\n",
    "        Note:\n",
    "            These params are fine tuned for Pong env.\n",
    "\n",
    "        Args:\n",
    "            arg_parser: parent parser\n",
    "        \"\"\"\n",
    "        arg_parser.add_argument(\n",
    "            \"--sync_rate\",\n",
    "            type=int,\n",
    "            default=1000,\n",
    "            help=\"how many frames do we update the target network\",\n",
    "        )\n",
    "        arg_parser.add_argument(\n",
    "            \"--replay_size\",\n",
    "            type=int,\n",
    "            default=100000,\n",
    "            help=\"capacity of the replay buffer\",\n",
    "        )\n",
    "        arg_parser.add_argument(\n",
    "            \"--warm_start_size\",\n",
    "            type=int,\n",
    "            default=10000,\n",
    "            help=\"how many samples do we use to fill our buffer at the start of training\",\n",
    "        )\n",
    "        arg_parser.add_argument(\n",
    "            \"--eps_last_frame\",\n",
    "            type=int,\n",
    "            default=150000,\n",
    "            help=\"what frame should epsilon stop decaying\",\n",
    "        )\n",
    "        arg_parser.add_argument(\"--eps_start\", type=float, default=1.0, help=\"starting value of epsilon\")\n",
    "        arg_parser.add_argument(\"--eps_end\", type=float, default=0.02, help=\"final value of epsilon\")\n",
    "        arg_parser.add_argument(\"--batches_per_epoch\", type=int, default=10000, help=\"number of batches in an epoch\")\n",
    "        arg_parser.add_argument(\"--batch_size\", type=int, default=32, help=\"size of the batches\")\n",
    "        arg_parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"learning rate\")\n",
    "\n",
    "        arg_parser.add_argument(\"--env\", type=str, required=True, help=\"gym environment tag\")\n",
    "        arg_parser.add_argument(\"--gamma\", type=float, default=0.99, help=\"discount factor\")\n",
    "\n",
    "        arg_parser.add_argument(\n",
    "            \"--avg_reward_len\",\n",
    "            type=int,\n",
    "            default=100,\n",
    "            help=\"how many episodes to include in avg reward\",\n",
    "        )\n",
    "        arg_parser.add_argument(\n",
    "            \"--n_steps\",\n",
    "            type=int,\n",
    "            default=1,\n",
    "            help=\"how many frames do we update the target network\",\n",
    "        )\n",
    "\n",
    "        return arg_parser\n",
    "    \n",
    "    def get_activations(self):\n",
    "        activations = collections.defaultdict(list)\n",
    "        def save_activation(name, mod, inp, out):\n",
    "            activations[name].append(out.cpu())\n",
    "\n",
    "        # Registering hooks for all the Conv2d layers\n",
    "        # Note: Hooks are called EVERY TIME the module performs a forward pass. For modules that are\n",
    "        # called repeatedly at different stages of the forward pass (like RELUs), this will save different\n",
    "        # activations. Editing the forward pass code to save activations is the way to go for these cases.\n",
    "        for name, m in net.named_modules():\n",
    "            if type(m)==nn.Conv2d:\n",
    "                # partial to assign the layer name to each hook\n",
    "                m.register_forward_hook(partial(save_activation, name))\n",
    "\n",
    "\n",
    "def cli_main():\n",
    "    parser = argparse.ArgumentParser(add_help=False)\n",
    "\n",
    "    # trainer args\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "\n",
    "    # model args\n",
    "    parser = DQN.add_model_specific_args(parser)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model = DQN(**args.__dict__)\n",
    "\n",
    "    # save checkpoints based on avg_reward\n",
    "    checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"avg_reward\", mode=\"max\", period=1, verbose=True)\n",
    "\n",
    "    seed_everything(123)\n",
    "    trainer = pl.Trainer.from_argparse_args(args, deterministic=True, checkpoint_callback=checkpoint_callback)\n",
    "\n",
    "    trainer.fit(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_net=DQN_(\"DemonAttack-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH='C:/Users/koester_lab/Documents/Maria/UdacityMachineLearningEngineerNanoDegree/dqn_model.pth'\n",
    "rl_net.net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH='C:/Users/koester_lab/Documents/Maria/UdacityMachineLearningEngineerNanoDegree/dqn_target_model.pth'\n",
    "rl_net.target_net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[130.0, 110.0, 310.0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env=\"DemonAttack-v0\"\n",
    "rl_net.run_n_episodes(env=rl_net.env,n_epsiodes=3,epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rl_net.net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
